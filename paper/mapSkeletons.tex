\section{Map-based Skeletons}
\label{sec:map-skeletons}
Now we have developed Parallel Arrows far enough to define some algorithmic skeletons useful to an application programmer.
\subsection{Parallel map}
\begin{center}
	\includegraphics[scale=0.7]{images/parMap}
\end{center}
\code{parMap} is probably the most common skeleton for parallel programs. We can implement it with \code{ArrowParallel} by repeating an arrow \code{arr a b} and then passing it into \code{parEvalN} to get an arrow \code{arr [a] [b]}. 
Just like \code{parEvalN}, \code{parMap} is 100 \% strict.
\begin{lstlisting}[frame=htrbl]
parMap :: (ArrowParallel arr a b conf) =>
	conf -> (arr a b) -> (arr [a] [b])
parMap conf f = parEvalN conf (repeat f)
\end{lstlisting}

\subsection{Lazy parallel map}
\begin{center}
	\includegraphics[scale=0.7]{images/parMapStream}
\end{center}
As \code{parMap} is 100\% strict it has the same restrictions as \code{parEvalN} compared to \code{parEvalNLazy}. So it makes sense to also have a \code{parMapStream} which behaves like \code{parMap}, but uses \code{parEvalNLazy} instead of \code{parEvalN}.
\begin{lstlisting}[frame=htrbl]
parMapStream :: (ArrowParallel arr a b conf, ArrowChoice arr, ArrowApply arr) =>
	conf -> ChunkSize -> arr a b -> arr [a] [b]
parMapStream conf chunkSize f = parEvalNLazy conf chunkSize (repeat f)
\end{lstlisting}

\subsection{Statically load-balancing parallel map}
\begin{center}
	\includegraphics[scale=0.7]{images/farm}
\end{center}
\code{parMap} spawns every single computation in a new thread (at least for the instances of \code{ArrowParallel} we gave in this paper). This can be quite wasteful and a \code{farm} that equally distributes the workload over \code{numCores} workers (if numCores is greater than actualProcessorCount, the fastest processor(s) to finish will get more tasks) seems useful.
\begin{lstlisting}[frame=htrbl]
farm :: (ArrowParallel arr a b conf,
	ArrowParallel arr [a] [b] conf, ArrowChoice arr) =>
	conf -> NumCores -> arr a b -> arr [a] [b]
farm conf numCores f =
	unshuffle numCores >>>
	parEvalN conf (repeat (mapArr f)) >>>
	shuffle
\end{lstlisting}

\subsection{farmChunk}
\begin{center}
	\includegraphics[scale=0.7]{images/farmChunk}
\end{center}
As \code{farm} is basically just \code{parMap} with a different work distribution, it is, again, 100\% strict. So we define \code{farmChunk} which uses \code{parEvalNLazy} instead of \code{parEvalN} like this:
\begin{lstlisting}[frame=htrbl]
farmChunk :: (ArrowParallel arr a b conf, ArrowParallel arr [a] [b] conf,
	ArrowChoice arr, ArrowApply arr) =>
	conf -> ChunkSize -> NumCores -> arr a b -> arr [a] [b]
farmChunk conf chunkSize numCores f =
	unshuffle numCores >>>
	parEvalNLazy conf chunkSize (repeat (mapArr f)) >>>
	shuffle
\end{lstlisting}

\subsection{parMapReduce}
\olcomment{it appears STRANGE. are the data really left alone and noded after map and taken from there by reduce? It makes sense only is no communication through master takes place. ELSE: CUT!}

-- this does not completely adhere to Google's definition of Map Reduce as it
-- the mapping function does not allow for "reordering" of the output
-- The original Google version can be found at https://de.wikipedia.org/wiki/MapReduce

\begin{lstlisting}[frame=htrbl]
parMapReduceDirect :: (ArrowParallel arr [a] b conf,
	ArrowApply arr, ArrowChoice arr) =>
	conf -> ChunkSize -> arr a b -> arr (b, b) b -> b -> arr [a] b
parMapReduceDirect conf chunkSize mapfn foldfn neutral =
	arr (chunksOf chunkSize) >>>
	parMap conf (mapArr mapfn >>> foldlArr foldfn neutral) >>>
	foldlArr foldfn neutral
\end{lstlisting}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
