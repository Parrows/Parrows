
To show that PArrows induce very small overhead even in a distributed context (and in general as well), we compare the original Eden
versions of the benchmark to its PArrows-enabled counterpart in the \rmtest, \torustest and \jacobitest benchmarks. We plot execution time differences between measurements for
PArrows and the corresponding backend in a separate plot
(Figs.~\ref{fig:bench-rm-dist}--\ref{fig:torusBenchmark}). As an example, the differences range in
about $0.5\seconds$ for the execution time of $46\seconds$ on 256 cores
for distributed \rmtest with PArrows and Eden. For these comparisons, the plots show absolute
time differences that are not relative \wrt the total execution time.
Furthermore, the error bars ends were computed from point-wise maximum of both standard
deviations from both measurements for PArrows and non-PArrows
versions. These are the values provided by the \ensuremath{\Varid{bench}} package that we
used for benchmarking. We call a difference between two versions
significant when the border of the error bar of absolute time
difference is above or below zero. In other words: the time
difference is significant if it is outside of the measurement error.

\subsubsection{\rmtest}\label{sec:rmtest}

\newcommand{\performanceSkelRMSM}[2]{
\performanceplot{Parallel run time of \rmtest \enquote{#2}}{Eden CP, GpH, \ensuremath{\Conid{Par}} Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-eden-cp-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-mult-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-par-#1-#2.csv};
}{17}{\plotwidthSMP}
}

\newcommand{\speedupSkelRMSM}[2]{
\speedupplot{Speedup of \rmtest \enquote{#2}}{Eden CP, GpH, \ensuremath{\Conid{Par}} Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-eden-cp-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-mult-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-par-#1-#2.csv};
}{17}{\plotwidthSMP}
}

\newcommand{\speedupSkelRMDist}[4]{
\speedupplot{Speedup of \rmtest \enquote{#1 #2}}{PArrows}{256}{#3}{
% \addplot [mark=*,very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-parrows-11213-#2.csv};
\addplot [mark=*,very thick,blue] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-parrows-#1-#2.csv};
% \addplot table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-eden-#1-#2.csv};
}{#4}{\plotwidthDist}
}

\newcommand{\performanceSkelRMDistDiff}[5]{
\performancediffplot{Overhead\\for \rmtest \enquote{#1 #2}}{(Eden $-$ PArrows)}{256}{#3}{
\addplot+[mark=*,very thick,error bars/.cd,
    y dir=both,y explicit] table [x="nCores", y="overhead", y error="stdDevForOverhead", col sep=comma, mark=dots,
smooth]{benchmarks/distributed-rm/#1-#2-diff.csv};
}{#4}{#5}{\plotwidthDist}
}

\begin{figure}
%\centering
%\performanceSkelRMSM{11213}{64}\hfill%
{\speedupSkelRMSM{11213}{32}}\hfill%
{\speedupSkelRMSM{11213}{64}}
\caption{Relative speedup of \rmtest on a multicore machine. We used the same PArrows-based implementation with
  different backends on the same hardware. Measurements were performed on a single node of the Glasgow
  cluster; it has 16 real cores. Input was $2^{11213}-1$, we used 32 (left) or 64 (right)
  tasks. The
  closer to linear speedup the better.}
\label{fig:bench-rm-sm}
\end{figure}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupSkelRMDist{44497}{256}{32}{272}\label{subfig:rm-dist-a}}%
%\hfill%
{\performanceSkelRMDistDiff{44497}{256}{32}{0.5}{272}\label{subfig:rm-dist-b}}
\caption{Parallel performance of the \rmtest on the Glasgow cluster
  consisting of 256 cores. Input was $2^{44497}-1$, we used 256
  tasks. The top plot shows absolute speedup in a distributed memory setting. The
  closer to linear speedup the better. Time
  (and hence speedup) measurements for PArrows with Eden backend and
  Eden almost coincide. Hence, bottom plot shows
absolute time differences for this benchmark. The
higher the value, the better for PArrows\olcomment{CHECKME}.}
\label{fig:bench-rm-dist}
\end{figure}

%\olcomment{THE ACTUAL TEXT IS MISSING. What do we see in the plots?
%  Why is it good?}
The multicore version of our parallel \rmtest benchmark is depicted in
Figure~\ref{fig:bench-rm-sm}. We executed the test with 32 and 64
tasks. The plot shows the PArrows-enabled versions with corresponding backends.
The performance of PArrows/Eden~CP in shared memory is slightly better than
for SMP variants such as PArrows/GpH and PArrows/\ensuremath{\Conid{Par}}
Monad but most of the time the performance is still comparable with the GpH backend performing slightly worse than the other two in terms of speedup.

Comparing the PArrows version of the \rmtest with the original from Eden with the MPI backend in a distributed memory setting, we see an almost linear speedup of
\rmtest with 256 tasks and input $2^{44497}-1$ in both versions. The sequential run time
was computed as the mean of three consecutive executions on a single
core---the single run took two hours 43 minutes. The difference between
PArrows/Eden and Eden almost always lies inside the error bar of
the measurement.

%As the PArrows version uses Eden in the backend, these numbers suggest that there is no real performance difference between using PArrows or Eden for this task as they trade blows in this benchmark. Additionally, PArrows with an Eden-based backend performing better than what it is based upon suggests that any difference in runtime between the two is more of an anomaly than a real difference.

\subsubsection{\jacobitest}

Continuing, the results of the \jacobitest (MPI only) in Fig.~\ref{fig:bench-jacobi-dist} are as follows:
The program does not seem to scale well beyond 64 threads with input $2^{3217}-1$. We once again compare the Eden version with the PArrrows version in Fig.~\ref{fig:bench-jacobi-dist}. We see similar behaviour to the MPI version of the \rmtest: The difference once again almost always remains within the bounds of the error bar of the measurement.

Because of the bad scaling behaviour for $2^{3217}-1$ beyond 64 threads, we also ran tests with input $2^{4253}-1$. Because of the long running time, we could do this only for the 128 and 256 threads. Therefore we do not show these results in Fig.~\ref{fig:bench-jacobi-dist} as we cannot properly compute a speedup without results for 1 thread. Nonetheless, for 128 threads, the benchmark took a mean of 9192.9\seconds, while with 256 threads, it only took a mean of 1649.1\seconds. This means that the 128 thread version ran more than 5.5 times slower than the 256 one, which suggests an IO limitation for this big input that is somewhat mitigated by adding more cores. However it still proves that PArrows continue to scale, even if not perfectly for this test program.
Comparing PArrows with Eden, for the larger input of $2^{4253}-1$, we see slightly bigger differences between Eden and PArrows: The differences for 128 and 256 core sare $-1005.18\seconds$ and $-94.33\seconds$ in favour of Eden, respectively. This maximum of $12.27\%$ slower PArrows runtime, however, was still in the (quite big) error bar of our measurements.

\newcommand{\speedupJacobiDist}[5]{
\speedupplot{Speedup of \jacobitest \enquote{#2} vs simulated \enquote{#5}}{PArrows #2, simulated PArrows #5}{256}{#3}{
% \addplot [mark=*,very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-parrows-11213-#2.csv};
\addplot [mark=*,very thick,blue] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/distributed-jacobi/bench-jacobi.bench.jacobi-parr-#1-#2.csv};
\addplot [mark=x,very thick,red] table [scatter, x="nCores", y="speedup"/, col sep=comma, mark=none,
smooth]{benchmarks/distributed-jacobi/bench-jacobi.bench.jacobi-parr-#1-#5.csv};
% \addplot table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-eden-#1-#2.csv};
}{#4}{\plotwidthDist}
}

\newcommand{\performanceJacobiDistDiff}[5]{
\performancediffplot{Overhead\\for \jacobitest \enquote{#2}}{(Eden $-$ PArrows) #2}{256}{#3}{
\addplot+[mark=*,very thick,error bars/.cd,
    y dir=both,y explicit] table [x="nCores", y="overhead", y error="stdDevForOverhead", col sep=comma, mark=dots,
smooth]{benchmarks/distributed-jacobi/#1-#2-diff.csv};
}{#4}{#5}{\plotwidthDist}
}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupJacobiDist{3}{3217}{32}{272}{4253}\label{subfig:jacobi-dist-a}}%
%\hfill%
{\performanceJacobiDistDiff{3}{3217}{32}{5}{272}\label{subfig:jacobi-dist-b}}
\caption{Parallel performance of the \jacobitest on the Glasgow cluster
  consisting of 256 cores. Input was $2^{3217}-1$, we used 256
  tasks. The top plot shows relative speedup in a distributed memory setting compared to a simulated speedup for input $2^{4253}-1$. The
  closer to linear speedup the better. Time
  (and hence speedup) measurements for PArrows with Eden backend and
  Eden almost coincide. Hence, bottom plot shows
absolute time differences for this benchmark. The
higher the value, the better for PArrows\olcomment{CHECKME}.}
\label{fig:bench-jacobi-dist}
\end{figure}

\subsubsection{\torustest}

Next is the \torustest benchmark. The results of the comparison of vanilla Eden to our PArrows-based version can be found in Fig.~\ref{fig:torusBenchmark}. We see that the benchmark scales quite well with more cores until 64 cores. For $>=96$ cores, we still have considerable speedup, but with less slope. We also prove that the difference between the Eden and PArrows version are only marginal with PArrows only being a maximum of $1.7\%$ slower -- for 160 cores -- when outside of the error bar.  %The difference between PArrows and Eden is only significant for 16 and 64 cores where it ran 1.7\% and 2.7\% slower which corresponds to a real-time difference of 0.12s and 0.13s. For 256 cores PArrows performed 0.2\% slower which corresponds to 0.01s overhead.

\newcommand{\speedupTorusDist}[3]{
\speedupplot{Speedup of \torustest \enquote{#1}}{PArrows}{256}{#2}{
\addplot [mark=*,very thick,blue] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/distributed-torus/bench-torus-distributed.bench.torus-matrix-parrows-#1.csv};
}{#3}{\plotwidthDist}
}


\newcommand{\performanceTorusDistDiff}[4]{
\performancediffplot{Overhead\\for \torustest \enquote{#1}}{(Eden $-$ PArrows)}{256}{#2}{
\addplot+[mark=*,very thick,error bars/.cd,
    y dir=both,y explicit] table [x="nCores", y="overhead", y error="stdDevForOverhead", col sep=comma, mark=dots,
smooth]{benchmarks/distributed-torus/#1-diff.csv};
}{#3}{#4}{\plotwidthDist}
}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupTorusDist{4096}{32}{272}\label{subfig:speedupTorusDist}}%
%\hfill%
{\performanceTorusDistDiff{4096}{32}{0.5}{272}\label{subfig:performancetorusDistDiff}}
\caption{Parallel performance of \torustest on the Glasgow cluster
  consisting of 256 cores. Input was a matrix size of $4096$. The top plot shows absolute speedup in a distributed memory setting. The
  closer to linear speedup the better. Time
  (and hence speedup) measurements for PArrows with Eden backend and
  Eden almost coincide. Hence, bottom plot shows
absolute time differences for this benchmark. The
higher the value, the better for PArrows\olcomment{CHECKME}.}
\label{fig:torusBenchmark}
\end{figure}


\subsubsection{\sudokutest}

As the last benchmark in this paper we present the \sudokutest in Fig.~\ref{fig:sudokuSMBenchmark} running in a shared memory setting. Here we see all three SM backends performing similarly again like in the \rmtest SM benchmarks in Figs.~\ref{fig:sudokuSMBenchmark} and \ref{fig:sudokuSMBenchmark16000}. However, we notice that the GpH backend seems to choke on a bigger input (Fig.~\ref{fig:sudokuSMBenchmark16000}). This is due to the benchmark only using \ensuremath{\Varid{parMap}} instead of a chunking variant -- however we did not change that for simplicity's sake. This issue is reflected by debug output which shows that of 16000 sparks being created (one for each Sudoku) only 8365 were converted (executed) with the rest (7635) overflowing the runtime spark pool. Another remarkable finding is that the Eden backend seems to lag behind for $\leq$16 threads, but manages to pull ahead noticeably with all 32 threads of the system in use.


\newcommand{\performanceSudokuSM}[1]{
\performanceplot{Parallel run time of \sudokutest \enquote{#1}}{linear speedup, Eden CP, GpH, \ensuremath{\Conid{Par}} Monad}{16}{4}{
\addplot [no markers,dotted,thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth] {benchmarks/sudoku-sm/bench-sudoku-sm.bench.fake-linear-sudoku-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-eden-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-mult-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-par-sudoku17.#1.txt.csv};
%\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
%smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parmonad-sudoku-sudoku17.#1.txt.csv};
}{17}{\plotwidthSMP}
}

\newcommand{\speedupSudokuSM}[1]{
\speedupplot{Parallel speedup of \sudokutest \enquote{#1}}{Eden CP, GpH, \ensuremath{\Conid{Par}} Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-eden-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-mult-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-par-sudoku17.#1.txt.csv};
%\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
%smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parmonad-sudoku-sudoku17.#1.txt.csv};
}{17}{\plotwidthSMP}
}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupSudokuSM{1000}\label{subfig:speedupSudokuSM}}%
%\hfill%
{\performanceSudokuSM{1000}\label{subfig:performanceSudokuSM}}
\caption{Absolute speedup of \sudokutest on a multicore machine. We used the same PArrows-based implementation with
  different backends on the same hardware and the \ensuremath{\Varid{parMap}} version from the \ensuremath{\Conid{Par}} Monad examples. Measurements were performed on a single node of the Glasgow
  cluster; it has 16 real cores and 32 threads. Input was a file of $1000$ Sudokus. The
  closer to linear speedup the better.}
\label{fig:sudokuSMBenchmark}
\end{figure}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupSudokuSM{16000}\label{subfig:speedupSudokuSM16000}}%
%\hfill%
{\performanceSudokuSM{16000}\label{subfig:performanceSudokuSM16000}}
\caption{Absolute speedup of \sudokutest on a multicore machine. We used the same PArrows-based implementation with
  different backends on the same hardware and the \ensuremath{\Varid{parMap}} version from the \ensuremath{\Conid{Par}} Monad examples. Measurements were performed on a single node of the Glasgow
  cluster; it has 16 real cores and 32 threads. Input was a file of $16000$ Sudokus. The
  closer to linear speedup the better. The GpH version shows signs of choking with too many sparks being created.}
\label{fig:sudokuSMBenchmark16000}
\end{figure}