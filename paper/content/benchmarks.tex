\section{Performance results}
\label{sec:benchmarks}

\newcommand{\rmtest}{Rabin--Miller test\xspace}
\newcommand{\sudokutest}{Sudoku\xspace}
\newcommand{\jacobitest}{Jacobi sum test\xspace}
\newcommand{\torustest}{Gentleman\xspace}
\newlength{\plotwidthSMP}
\setlength{\plotwidthSMP}{0.39\textwidth}
\newlength{\plotwidthDist}
\setlength{\plotwidthDist}{0.6\textwidth}


\newcommand{\performanceplot}[7]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#7,
xlabel=Threads,
%xtick=data,
xtick distance=#4,
ylabel=Time (s),
ylabel near ticks,
minor tick num=2,
grid=major,
legend entries={#2},
legend style={at={(0.99,0.99)},anchor=north east},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
xmin=-1,
xmax=#6]
#5
\end{axis}
\end{tikzpicture}
}

\newcommand{\performancediffplot}[8]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#8,
xlabel=Threads,
%xtick=data,
ytick distance=#6,
xtick distance=#4,
minor tick num=9,
ylabel=Absolute time difference (s),
ylabel near ticks,
grid=both,
legend entries={#2},
legend style={at={(0.99,0.99)},anchor=north east},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
xmin=-1,
xmax=#7]
#5
\end{axis}
\end{tikzpicture}
}

\newcommand{\speedupplot}[7]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#7,
xlabel=Threads,
%xtick=data,
%ytick=data,
xtick distance=#4,
ytick distance=#4,
ylabel=Speedup,
ylabel near ticks,
grid=major,
legend entries={linear, #2},
legend style={at={(0.01,0.99)},anchor=north west},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
ymin=-1,
xmin=-1,
ymax=#6,
xmax=#6]
\addplot [domain=0:#3, no markers,dotted,thick]{x};
#5
\end{axis}
\end{tikzpicture}
}

\newcommand{\speedupdiffplot}[7]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#7,
xlabel=Threads,
%xtick=data,
xtick distance=#4,
ytick distance=0.5,
ylabel=Absolute speedup difference,
ylabel near ticks,
grid=major,
legend entries={#2},
legend style={at={(0.99,0.99)},anchor=north east},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
xmin=-1,
xmax=#6]
#5
\end{axis}
\end{tikzpicture}
}

\subsection{Hardware}

We have tested our parallel DSL and algorithmic skeletons implemented
in it. Benchmarks were conducted both in a shared and in a distributed
memory setting. All benchmarks were done on the ``Glasgow grid'', consisting of
16 machines with 2 Intel\SymbReg~Xeon\SymbReg~E5-2640 v2 and 64 GB of DDR3 RAM each. Each processor has 8 cores and 16 (hyperthreaded) threads with a base frequency of 2 GHz and a turbo frequency of 2.50 GHz. This results in a total of 256 cores and 512 threads for the whole cluster. The operating system was Ubuntu 14.04 LTS with Kernel 3.19.0-33. Non-surprisingly, we found that hyperthreaded 32 cores do not behave in the same manner as real 16 cores (numbers here for a single machine). We disregarded the hyperthreading ability in most of the
cases.

We used a single node with 16 real cores as a shared memory testbed
and the whole grid with 256 real cores as a device to test our
distributed memory software.

\subsection{Test programs}

We used multiple tests that originated from different
sources. Most of them are parallel mathematical computations, initially
implemented in Eden. Table~\ref{tab:benches} summarizes.

\begin{table}
\caption{The benchmarks we use in this paper.}
\label{tab:benches}
\centering
%% something was wrong with separators in table
\renewcommand{\tabcolsep}{1em}
\begin{tabular}{lccll}
\toprule
Name & Area & Type & Origin & Citation \\
\midrule
\rmtest & Mathematics & |parMap+reduce| & Eden & \citet{Lobachev2012}\\
\jacobitest & Mathematics & |workpool+reduce| & Eden & \citet{Lobachev2012}\\
\torustest & Mathematics & |torus| & Eden & \citet{Eden:SkeletonBookChapter02}\\
\sudokutest & Puzzle & |parMap| & |Par| Monad & \citet{par-monad} 
\tablefootnote{actual code from: http://community.haskell.org/~simonmar/par-tutorial.pdf and https://github.com/simonmar/parconc-examples}\\
\bottomrule
\end{tabular}
\end{table}

\rmtest is a probabilistic primality test that iterates multiple (32--256 here)
``subtests''. Should a subtest fail, the input is definitely not a
prime. If all $n$ subtest pass, the input is composite with the
probability of $1/4^{n}$. 

Jacobi sum test or APRCL is also a primality test, that however,
guarantees the correctness of the result. It is probabilistic in the
sense that its run time is not certain. Unlike \rmtest, the subtests
of Jacobi sum test have very different durations. \citet{lobachev-phd}
\olcomment{discuss some optimisations of parallel APRCL.} Generic parallel
implementation of \rmtest and APRCL were presented in \citet{Lobachev2012}.

``Gentleman'' is a standard Eden test program, developed
for their |torus| skeleton. It implements a parallel matrix
multiplication \citep{Gentleman1978}. We ported an Eden based version \citep{Eden:SkeletonBookChapter02} to PArrows.

A~parallel Sudoku solver was used by \citet{par-monad} to compare |Par| Monad
to Multicore Haskell. We ported it to PArrows.



\subsection{What parallel Haskells run where}

The \ensuremath{\Conid{Par}} monad and Multicore Haskell can be executed on a shared
memory machines only. Although GpH is available on distributed memory
clusters, and newer distributed memory Haskells such as HdpH exist,
current support of distributed memory in PArrows is limited to
Eden. We used the MPI backend of Eden in a distributed memory
setting. However, for shared memory Eden features a ``CP'' backend
that merely copies the memory blocks between distributed heaps. In
this mode Eden still operates in the ``nothing shared'' setting, but
is adapted better to multicore machines. We label this version of Eden
in the plots as ``Eden~CP''.



\subsection{Effect of hyperthreading}

The PArrows version of \rmtest on a single node of the Glasgow grid
showed almost linear speedup (Fig.~\ref{fig:bench-rm-sm}). The speedup
of 64-task PArrows/Eden at 16 real cores version was 13.65, the
efficiency was 85.3\%.  However, if we increase the number of
requested cores to be 32---\ie if we use hyperthreading on 16 real
cores---the speedup does not increase that well. It is merely 15.99
for 32 tasks with PArrows/Eden. It is worse for other backends.  As
for 64 tasks, we obtain the speedup of 16.12 with PArrows/Eden at 32
hyperthreaded cores and only 13.55 with PArrows/Multicore
Haskell. Efficiency is 50.4\% and 42.3\%, respectively. The Eden
version used here was Eden~CP, the ``share nothing'' SMP build.

In the distributed memory setting the same effect ensues. We obtain
plummeting speedup of 124.31 at 512 hyperthreaded cores, whereas it was
213.172 for 256 real cores. Apparently, hyperthreading in the Glasgow
grid fails to execute two parallel Haskell processes with full-fledged
parallelism. For this reason, we did not regard hyperthreaded cores in
our speedup plots in Figs.~\ref{fig:bench-rm-sm}-\ref{fig:sudokuSMBenchmark}.


% rm 11213 32 32-sm speedup eden: 15.993037587283924
% -"- multi: 15.09948017762912
% -"- par: 14.909092857846693

% -"- 64 32-sm speedup eden: 16.118040224478424
% -"- multi: 13.545304115702333
% -"- par: 15.155709987503396

\subsection{Benchmark results}

The difference between, say, PArrows with |Par| Monad backend and a
genuine |Par|
Monad benchmark is very small. To give an example, it is ~0.4s in favour of PArrows for 16 cores (10.8s vs. 11.2s) and ~-0.8s in favour of the |Par| monad for 8 cores (16.1s vs. 16.9s) for
the Sudoku benchmark in the shared memory setting. It is almost invisible in speedup and
(non shown) run time plots. We thus show only the results for the
PArrows-enabled versions.

To show that PArrows induce very small overhead in a distributed context as well, we compare the original Eden
versions of the benchmark to its PArrows-enabled counterpart in the \rmtest, \torustest and \jacobitest benchmarks. We plot execution time differences between measurements for
PArrows and the corresponding backend in a separate plot
(Figs.~\ref{fig:bench-rm-dist}, \ref{fig:torusBenchmark}, \ref{fig:bench-jacobi-dist}). As an example, the differences range in
about 0.5~seconds for the execution time of 46~seconds on 256 cores
for distributed \rmtest with PArrows and Eden. For these comparisons, the plots show absolute
time differences that are not relative \wrt the total execution time.
Furthermore, the error bars ends were computed from pointwise maximum of both standard
deviations from both measurements for PArrows and non-PArrows
versions. These are the values provided by the |bench| package that we
used for benchmarking. We call a difference between two versions
significant when the border of the error bar of absolute time
difference is above or below zero. In other words: the time
difference is significant if it is above measurement error.

\subsubsection{\rmtest}\label{sec:rmtest}

\newcommand{\performanceSkelRMSM}[2]{
\performanceplot{Parallel run time of \rmtest ``#2''}{Eden CP, Multicore, |Par| Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-eden-cp-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-mult-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-par-#1-#2.csv};
}{17}{\plotwidthSMP}
}

\newcommand{\speedupSkelRMSM}[2]{
\speedupplot{Speedup of \rmtest ``#2''}{Eden CP, Multicore, |Par| Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-eden-cp-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-mult-#1-#2.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sm-rm/bench-sm-rm.bench.skelrm-parr-par-#1-#2.csv};
}{17}{\plotwidthSMP}
}

\newcommand{\speedupSkelRMDist}[4]{
\speedupplot{Speedup of \rmtest ``#2''}{PArrows}{256}{#3}{
% \addplot [mark=*,very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-parrows-11213-#2.csv};
\addplot [mark=*,very thick,blue] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-parrows-#1-#2.csv};
% \addplot table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-eden-#1-#2.csv};
}{#4}{\plotwidthDist}
}

\newcommand{\performanceSkelRMDistDiff}[5]{
\performancediffplot{Run time differences\\for \rmtest ``#2''}{(Eden $-$ PArrows)}{256}{#3}{
\addplot+[mark=*,very thick,error bars/.cd,
    y dir=both,y explicit] table [x="nCores", y="time", y error="max stddev", col sep=comma, mark=dots,
smooth]{benchmarks/distributed-rm/#1-#2-diff.csv};
}{#4}{#5}{\plotwidthDist}
}

\begin{figure}
%\centering
%\performanceSkelRMSM{11213}{64}\hfill%
{\speedupSkelRMSM{11213}{32}}\hfill%
{\speedupSkelRMSM{11213}{64}}
\caption{Relative speedup of \rmtest on a multicore machine. We used the same PArrows-based implementation with
  different backends on the same hardware. Measurements were performed on a single node of the Glasgow
  grid; it has 16 real cores. Input was $2^{11213}-1$, we used 32 (left) or 64 (right)
  tasks. The
  closer to linear speedup the better.}
\label{fig:bench-rm-sm}
\end{figure}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupSkelRMDist{44497}{256}{32}{260}\label{subfig:rm-dist-a}}%
%\hfill%
{\performanceSkelRMDistDiff{44497}{256}{32}{0.5}{260}\label{subfig:rm-dist-b}}
\caption{Parallel performance of \rmtest on the Glasgow grid
  consisting of 256 cores. Input was $2^{44497}-1$, we used 256
  tasks. The top plot shows absolute speedup in a distributed memory setting. The
  closer to linear speedup the better. Time
  (and hence speedup) measurements for PArrows with Eden backend and
  Eden almost coincide. Hence, bottom plot shows
absolute time differences for this benchmark. The
higher the value, the better for PArrows\olcomment{CHECKME}.}
\label{fig:bench-rm-dist}
\end{figure}

\olcomment{THE ACTUAL TEXT IS MISSING. What do we see in the plots?
  Why is it good?}
The multicore version of our parallel \rmtest benchmark is depicted in
Figure~\ref{fig:bench-rm-sm}. We executed the test with 32 and 64
tasks. The plot shows the PArrows-enabled versions with corresponding backends.
The performance of PArrows/Eden~CP in shared memory is slightly better than
for SMP variants such as PArrows/Multicore Haskell and PArrows/|Par|
Monad but most of the time the performance is still comparable with the Multicore backend performing slightly worse than the other two in terms of speedup. In particular, the speedups for 16 cores fall behind quite noticeably with 11.69 for 32 tasks as Eden and the |Par| Monad both score 13.05 for 32 tasks. While the Multicore backend scores a bit better in the 64 tasks benchmark, its speedup of 12.70 is still behind the 13.65 and 13.33 of Eden and |Par| Monad, respectively.

Comparing the PArrows version of the \rmtest with the original from Eden with the MPI backend in a distributed memory setting, we see an almost linear speedup of
\rmtest with 256 tasks and input $2^{444497}-1$ in both versions. The sequential run time
was computed as the mean of three consecutive executions on a single
core---the single run took two hours 43 minutes. The zero difference  between
PArrows/Eden and Eden almost always lies on the error bar of
the measurement. The only exception, where the PArrows and Eden versions differ and it was it was significant, was for 64 cores, where it performed 0.49s better. This corresponds to
0.30\% relative time difference. Otherwise, PArrows was 0.23s (or 0.0007\%) slower for 32 cores, 0.26s (or 0.0030\%) slower for 128 cores, and 0.09s faster (or 0.0019\%) faster for 256 cores.

%As the PArrows version uses Eden in the backend, these numbers suggest that there is no real performance difference between using PArrows or Eden for this task as they trade blows in this benchmark. Additionally, PArrows with an Eden-based backend performing better than what it is based upon suggests that any difference in runtime between the two is more of an anomaly than a real difference.

\subsubsection{\jacobitest}

Continuing, the results of the \jacobitest in \ref{fig:bench-jacobi-dist} are as follows:

\newcommand{\speedupJacobiDist}[4]{
\speedupplot{Speedup of \jacobitest ``#2''}{PArrows}{256}{#3}{
% \addplot [mark=*,very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-parrows-11213-#2.csv};
\addplot [mark=*,very thick,blue] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/distributed-jacobi/bench-jacobi.bench.jacobi-parr-#1-#2.csv};
% \addplot table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
% smooth]{benchmarks/distributed-rm/bench-distributed.bench.skelrm-eden-#1-#2.csv};
}{#4}{\plotwidthDist}
}

\renewcommand{\speedupJacobiDist}[4]{
	\textcolor{red}{benchmark missing. in the works}
}

\newcommand{\performanceJacobiDistDiff}[5]{
\performancediffplot{Run time differences\\for \jacobitest ``#2''}{(Eden $-$ PArrows)}{256}{#3}{
\addplot+[mark=*,very thick,error bars/.cd,
    y dir=both,y explicit] table [x="nCores", y="time", y error="max stddev", col sep=comma, mark=dots,
smooth]{benchmarks/distributed-jacobi/#1-#2-diff.csv};
}{#4}{#5}{\plotwidthDist}
}

\renewcommand{\performanceJacobiDistDiff}[5]{
	\textcolor{red}{benchmark missing. in the works}
}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupJacobiDist{3}{1279}{32}{260}\label{subfig:rm-dist-a}}%
%\hfill%
{\performanceJacobiDistDiff{3}{1279}{32}{0.5}{260}\label{subfig:rm-dist-b}}
\caption{Parallel performance of \rmtest on the Glasgow grid
  consisting of 256 cores. Input was $2^{1279}-1$, we used 256
  tasks. The top plot shows absolute speedup in a distributed memory setting. The
  closer to linear speedup the better. Time
  (and hence speedup) measurements for PArrows with Eden backend and
  Eden almost coincide. Hence, bottom plot shows
absolute time differences for this benchmark. The
higher the value, the better for PArrows\olcomment{CHECKME}.}
\label{fig:bench-jacobi-dist}
\end{figure}

\mbcomment{FIXME: redo this test with $2^{44497}$}

\subsubsection{\torustest}

Next is the \torustest benchmark. The results of the comparison of vanilla Eden to our PArrows-based version can be found in \ref{fig:torusBenchmark}. While we do not see too much speedup for more than 16 cores, we can prove that the difference between the Eden and PArrows version are again marginal. The difference between PArrows and Eden is only significant for 16 and 64 cores where it ran 1.7\% and 2.7\% slower which corresponds to a real-time difference of 0.12s and 0.13s. For 256 cores PArrows performed 0.2\% slower which corresponds to 0.01s overhead.

\newcommand{\speedupTorusDist}[3]{
\speedupplot{Speedup of \torustest ``#1''}{PArrows}{256}{#2}{
\addplot [mark=*,very thick,blue] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/distributed-torus/bench-torus-distributed.bench.torus-matrix-parrows-#1.csv};
}{#3}{\plotwidthDist}
}

\renewcommand{\speedupTorusDist}[3]{
	\textcolor{red}{benchmark missing. in the works}
}


\newcommand{\performanceTorusDistDiff}[4]{
\performancediffplot{Run time differences\\for \torustest ``#1''}{(Eden $-$ PArrows)}{256}{#2}{
\addplot+[mark=*,very thick,error bars/.cd,
    y dir=both,y explicit] table [x="nCores", y="time", y error="max stddev", col sep=comma, mark=dots,
smooth]{benchmarks/distributed-torus/#1-diff.csv};
}{#3}{#4}{\plotwidthDist}
}

\renewcommand{\performanceTorusDistDiff}[4]{
		\textcolor{red}{benchmark missing. in the works}
}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupTorusDist{1024}{32}{260}\label{subfig:speedupTorusDist}}%
%\hfill%
{\performanceTorusDistDiff{1024}{32}{0.5}{260}\label{subfig:performancetorusDistDiff}}
\caption{Parallel performance of \torustest on the Glasgow grid
  consisting of 256 cores. Input was a matrix size of $1024$. The top plot shows absolute speedup in a distributed memory setting. The
  closer to linear speedup the better. Time
  (and hence speedup) measurements for PArrows with Eden backend and
  Eden almost coincide. Hence, bottom plot shows
absolute time differences for this benchmark. The
higher the value, the better for PArrows\olcomment{CHECKME}.}
\label{fig:torusBenchmark}
\end{figure}


\subsubsection{\sudokutest}

As the last Benchmark in this paper we present the \sudokutest in \ref{fig:sudokuSMBenchmark} running in a shared memory setting. Here we see all three SM backends performing similarly again like in the \rmtest SM benchmarks in Figs.~\ref{fig:sudokuSMBenchmark}-\ref{fig:sudokuSMBenchmark16000}. However, we notice that the Multicore backend seems to choke on a bigger input \ref{fig:sudokuSMBenchmark16000}. This is due to the benchmark only using |parMap| instead of a chunking variant (however we did not change that for simplicity's sake) and is reflected by debug output which shows, that of 16000 sparks being created (one for each sudoku) only 8365 were converted (executed) with the rest (7635) overflowing the runtime spark pool. Another remarkable finding is that the Eden backend seems to lack behind for $\leq$16 threads, but manages to pull ahead noticeably with all 32 threads of the system in use.


\newcommand{\performanceSudokuSM}[1]{
\performanceplot{Parallel run time of \sudokutest ``#1''}{Eden CP, Multicore, |Par| Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-eden-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-mult-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-par-sudoku17.#1.txt.csv};
%\addplot+ [very thick] table [scatter, x="nCores", y="time", col sep=comma, mark=none,
%smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parmonad-sudoku-sudoku17.#1.txt.csv};
}{17}{\plotwidthSMP}
}

\newcommand{\speedupSudokuSM}[1]{
\speedupplot{ Parallel speedup of \sudokutest ``#1''}{Eden CP, Multicore, |Par| Monad}{16}{4}{
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-eden-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-mult-sudoku17.#1.txt.csv};
\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parrows-sudoku-parmap-par-sudoku17.#1.txt.csv};
%\addplot+ [very thick] table [scatter, x="nCores", y="speedup", col sep=comma, mark=none,
%smooth]{benchmarks/sudoku-sm/bench-sudoku-sm.bench.parmonad-sudoku-sudoku17.#1.txt.csv};
}{17}{\plotwidthSMP}
}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupSudokuSM{1000}\label{subfig:speedupSudokuSM}}%
%\hfill%
{\performanceSudokuSM{1000}\label{subfig:performanceSudokuSM}}
\caption{Relative speedup of \sudokutest on a multicore machine. We used the same PArrows-based implementation with
  different backends on the same hardware and the |parMap| version from the |Par| Monad examples. Measurements were performed on a single node of the Glasgow
  grid; it has 16 real cores and 32 threads. Input was a file of $1000$ Sudokus. The
  closer to linear speedup the better.}
\label{fig:sudokuSMBenchmark}
\end{figure}

\begin{figure}
\centering
%\performanceSkelRMDist{44497}{256}{32,64,128,256,512}{544}
%
{\speedupSudokuSM{16000}\label{subfig:speedupSudokuSM16000}}%
%\hfill%
{\performanceSudokuSM{16000}\label{subfig:performanceSudokuSM16000}}
\caption{Relative speedup of \sudokutest on a multicore machine. We used the same PArrows-based implementation with
  different backends on the same hardware and the |parMap| version from the |Par| Monad examples. Measurements were performed on a single node of the Glasgow
  grid; it has 16 real cores and 32 threads. Input was a file of $16000$ Sudokus. The
  closer to linear speedup the better. The Multicore version shows signs of choking with too many sparks being created.}
\label{fig:sudokuSMBenchmark16000}
\end{figure}
