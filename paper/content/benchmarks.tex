
%
\section{Performance results and discussion}
\label{sec:benchmarks}

In the following section, we describe the benchmarks of our parallel DSL and algorithmic skeletons conducted.
We start by explaining the hardware and software stack and also elaborate on which benchmarks programs were used
and also which parallel Haskell were used in which setting and why. Before we go into detail on the benchmarks we
also address the issue of poor performance gains when using hyper-threading and our reasoning behind not including
hyper-threaded cores in our benchmarks. Finally, we show that PArrows hold up in terms of performance when compared to the original parallel Haskells used as backends in this paper, starting with the shared-memory variants (GpH, |Par| Monad and Eden CP) and concluding with Eden as a distributed backend.


\newcommand{\rmtest}{Rabin--Miller test\xspace}
\newcommand{\sudokutest}{Sudoku\xspace}
\newcommand{\jacobitest}{Jacobi sum test\xspace}
\newcommand{\torustest}{Gentleman\xspace}
\newlength{\plotwidthSMP}
\setlength{\plotwidthSMP}{0.39\textwidth}
\newlength{\plotwidthDist}
\setlength{\plotwidthDist}{0.6\textwidth}


\newcommand{\performanceplot}[7]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#7,
xlabel=Threads,
%xtick=data,
xtick distance=#4,
ylabel=Time (s),
ylabel near ticks,
minor tick num=2,
grid=major,
legend entries={#2},
legend style={at={(0.99,0.99)},anchor=north east},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
xmin=-1,
xmax=#6]
#5
\end{axis}
\end{tikzpicture}
}

\newcommand{\performancediffplot}[8]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#8,
xlabel=Threads,
%xtick=data,
ytick distance=#6,
xtick distance=#4,
minor tick num=9,
ylabel=Absolute time difference (s),
ylabel near ticks,
grid=both,
legend entries={#2},
legend style={at={(0.99,0.99)},anchor=north east},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
xmin=-1,
xmax=#7]
#5
\end{axis}
\end{tikzpicture}
}

\newcommand{\speedupplot}[7]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#7,
xlabel=Threads,
%xtick=data,
%ytick=data,
xtick distance=#4,
ytick distance=#4,
ylabel=Speedup,
ylabel near ticks,
grid=major,
legend entries={linear, #2},
legend style={at={(0.01,0.99)},anchor=north west},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
ymin=-1,
xmin=-1,
ymax=#6,
xmax=#6]
\addplot [domain=0:#3, no markers,dotted,thick]{x};
#5
\end{axis}
\end{tikzpicture}
}

\newcommand{\speedupdiffplot}[7]{
\begin{tikzpicture}
\begin{axis}[title={#1},
title style={align=center},
scale only axis, width=#7,
xlabel=Threads,
%xtick=data,
xtick distance=#4,
ytick distance=0.5,
ylabel=Absolute speedup difference,
ylabel near ticks,
grid=major,
legend entries={#2},
legend style={at={(0.99,0.99)},anchor=north east},
max space between ticks=50pt,
grid style={line width=.1pt, draw=gray!10},
major grid style={line width=.2pt,draw=gray!50},
xmin=-1,
xmax=#6]
#5
\end{axis}
\end{tikzpicture}
}

\subsection{Preliminaries}

\subsubsection{Hardware and Software used}

Benchmarks were run both in a shared and in a distributed
memory setting. All benchmarks were done on the Glasgow GPG Beowulf cluster, consisting of
16 machines with 2 Intel\SymbReg~Xeon\SymbReg~E5-2640 v2 and 64 GB of DDR3 RAM each. Each processor has 8 cores and 16 (hyper-threaded) threads with a base frequency of 2 GHz and a turbo frequency of 2.50 GHz. This results in a total of 256 cores and 512 threads for the whole cluster. The operating system was Ubuntu 14.04 LTS with Kernel 3.19.0-33. Non-surprisingly, we found that hyper-threaded 32 cores do not behave in the same manner as real 16 cores (numbers here for a single machine). We disregarded the hyper-threading ability in most of the cases.

Apart from Eden, all benchmarks and libaries were compiled with Stack's\footnote{see \url{https://www.haskellstack.org/}} lts-7.1 GHC compiler which is equivalent to a standard GHC 8.0.1 with the base package in version 4.9.0.0. Stack itself was used in version 1.3.2. For GpH in its Multicore variant we used the the parallel package in version 3.2.1.0\footnote{see \url{https://hackage.haskell.org/package/parallel-3.2.1.0}}, while for the |Par| monad we used monad-par in version 0.3.4.8\footnote{see \url{https://hackage.haskell.org/package/monad-par-0.3.4.8}}. For all Eden tests, we used its GHC-Eden compiler in version 7.8.2\footnote{see \url{http://www.mathematik.uni-marburg.de/~eden/?content=build_eden_7_&navi=build}} together with OpenMPI 1.6.5\footnote{see \url{https://www.open-mpi.org/software/ompi/v1.6/}}.

Furtermore, all benchmarks were done with help of the bench\footnote{see \url{https://hackage.haskell.org/package/bench}} tool in version 1.0.2 which uses criterion (>=1.1.1.0 \&\& < 1.2)\footnote{see \url{https://hackage.haskell.org/package/criterion-1.1.1.0}} internally. All runtime data (mean runtime, max stddev, etc.) was collected with this tool if not mentioned otherwise.

We used a single node with 16 real cores as a shared memory testbed
and the whole grid with 256 real cores as a device to test our
distributed memory software.

\subsubsection{Benchmarks}

We used multiple tests that originated from different
sources. Most of them are parallel mathematical computations, initially
implemented in Eden. Table~\ref{tab:benches} summarises.

\begin{table}
\centering
\caption{The benchmarks we use in this paper.}
\label{tab:benches}
%% something was wrong with separators in table
\renewcommand{\tabcolsep}{0.5em}
\begin{tabular}{lccll}
\toprule
Name & Area & Type & Origin & Source \\
\midrule
\rmtest & Mathematics & \ensuremath{\Varid{parMap}\mathbin{+}\Varid{reduce}} & Eden & \citet{Lobachev2012}\\
\jacobitest & Mathematics & \ensuremath{\Varid{workpool}\mathbin{+}\Varid{reduce}} & Eden & \citet{Lobachev2012}\\
\torustest & Mathematics & \ensuremath{\Varid{torus}} & Eden & \citet{Eden:SkeletonBookChapter02}\\
\sudokutest & Puzzle & \ensuremath{\Varid{parMap}} & \ensuremath{\Conid{Par}} Monad & \citet{par-monad}\tablefootnote{actual code from: \url{http://community.haskell.org/\~simonmar/par-tutorial.pdf and https://github.com/simonmar/parconc-examples}}\\
\bottomrule
\end{tabular}
\end{table}

\rmtest is a probabilistic primality test that iterates multiple (here: 32--256)
\enquote{subtests}. Should a subtest fail, the input is definitely not a
prime. If all $n$ subtest pass, the input is composite with the
probability of $1/4^{n}$. 

Jacobi sum test or APRCL is also a primality test, that however,
guarantees the correctness of the result. It is probabilistic in the
sense that its run time is not certain. Unlike \rmtest, the subtests
of Jacobi sum test have very different durations. \citet{lobachev-phd}
discusses some optimisations of parallel APRCL. Generic parallel
implementations of \rmtest and APRCL were presented in \citet{Lobachev2012}.

\enquote{Gentleman} is a standard Eden test program, developed
for their \ensuremath{\Varid{torus}} skeleton. It implements a Gentleman's algorithm for parallel matrix
multiplication \citep{Gentleman1978}. We ported an Eden based version \citep{Eden:SkeletonBookChapter02} to PArrows.

A~parallel Sudoku solver was used by \citet{par-monad} to compare \ensuremath{\Conid{Par}} Monad
to GpH, and we ported it to PArrows.

\input{bestAndWorstBenchmarks}

\subsubsection{What parallel Haskells run where}

The \ensuremath{\Conid{Par}} monad and GpH -- in its multicore version \cite{Marlow2009} --  can be executed on shared memory machines only.
Although GpH is available on distributed memory
clusters, and newer distributed memory Haskells such as HdpH exist,
current support of distributed memory in PArrows is limited to
Eden. We used the MPI backend of Eden in a distributed memory
setting. However, for shared memory Eden features a ``CP'' backend
that merely copies the memory blocks between distributed heaps. In
this mode, Eden still operates in the ``nothing shared'' setting, but
is adapted better to multicore machines. We label this version of Eden
in the plots as ``Eden~CP''.



\subsubsection{Effect of hyper-threading}

In preliminary tests, the PArrows version of \rmtest on a single node of the Glasgow cluster
showed almost linear speedup on up to 16 shared-memory cores (Fig.~\ref{fig:bench-rm-sm}). The speedup
of 64-task PArrows/Eden at 16 real cores version was 13.65 giving a parallel
efficiency of 85.3\%. However, if we increased the number of
requested cores to 32 -- \ie if we use hyper-threading on 16 real
cores -- the speedup did not increase that well. It was merely 15.99
for 32 tasks with PArrows/Eden. This was worse for other backends.  As
for 64 tasks, we obtained a speedup of 16.12 with PArrows/Eden at 32
hyper-threaded cores and only 13.55 with PArrows/GpH. The parallel eficiencies were 50.4\% and 42.3\%, respectively. The Eden
version used here was Eden~CP, the \enquote{share nothing} SMP build.

In the distributed memory setting the same effect ensues. We obtain
plummeting speedup of 124.31 at 512 hyper-threaded cores, whereas it was
213.172 for 256 real cores. Apparently, hyper-threading in the Glasgow
cluster fails to execute two parallel Haskell processes with full-fledged
parallelism.

\mbcomment{fix this when benchmarks are redone}
Hence, the performance measurements in Figs.~\ref{fig:bench-rm-sm}--\ref{fig:sudokuSMBenchmark} and Table~\ref{tab:meanOverhead} use only real cores as the purpose of this paper is to show the performance of PArrows and not to investigate parallel performance with hyper-threading in use.


% rm 11213 32 32-sm speedup eden: 15.993037587283924
% -"- multi: 15.09948017762912
% -"- par: 14.909092857846693

% -"- 64 32-sm speedup eden: 16.118040224478424
% -"- multi: 13.545304115702333
% -"- par: 15.155709987503396

\subsection{Benchmark results}

In the following paragraphs we will go into detail on how well PArrows perform when compared to versions of the benchmark programs implemented with the original parallel Haskells that were used in the backends. We start by comparing Speedups and Overheads of for the shared memory backends and conclude with OpenMPI variants of the Eden-enabled backend as a representative of a distributed memory backend.

\subsubsection{Shared memory}

\paragraph{Speedup}

\paragraph{Overhead}

\subsubsection{Distributed Memory}

\paragraph{Speedup}

\paragraph{Overhead}


\subsection{Discussion}

In the shared memory setting we naturally find that the backends perform differently. Furthermore, in our distributed memory tests on the full grid the difference between PArrows and Eden was almost always below the error margin if PArrows performed worse. Even the biggest difference of 12.27\% in the \jacobitest for input $2^{4253}-1$ and 128 cores remained in line with these findings. The only exception to this was the \torustest  with only a $1.7\%$ difference in favour of Eden which is only a marginal slowdown in our opinion.
